{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc521175",
   "metadata": {},
   "source": [
    "# Agent Lab Draft 3 - Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9294896a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "\n",
    "from autogen import ConversableAgent, AssistantAgent\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# Mostly just code from lab 3 packaged into functions\n",
    "import lab3code as lab3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea23835",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b16d0eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class mnistClassification(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim): # Feel free to add hidden_dim as parameters here\n",
    "        super(mnistClassification, self).__init__()\n",
    "        \n",
    "        self.description = \"Simple FCN with 2 layers, ReLU activation, and dropout\"\n",
    "        \n",
    "        #self.layerconv1 = torch.nn.Conv1d(in_channels=1, out_channels=1, kernel_size=5, stride=2, padding=2)\n",
    "        self.layer1 = torch.nn.Linear(input_dim, 200) # First layer\n",
    "        self.layer2 = torch.nn.Linear(200, output_dim) # Second layer\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dropout = torch.nn.Dropout(p=0.2) # Dropout layer with 20% dropout rate\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = torch.nn.functional.softmax(x, dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2718ca2",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30552c0",
   "metadata": {},
   "source": [
    "### DatasetLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55be5014",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetLoaderAgent(AssistantAgent):\n",
    "    \"\"\"\n",
    "    An agent that specializes in loading and preprocessing datasets.\n",
    "    \"\"\"\n",
    "    def __init__(self, name=\"DatasetLoader\", **kwargs):\n",
    "        self.available_datasets = [\"mnist\", \"solar_flare\"]\n",
    "        system_message = (\n",
    "            \"I am a dataset loading assistant. I can load and preprocess various datasets for machine learning tasks. \"\n",
    "            f\"Currently I support: {', '.join(self.available_datasets)}. \"\n",
    "        )\n",
    "        super().__init__(name=name, system_message=system_message, **kwargs)\n",
    "        \n",
    "        # Store loaded datasets\n",
    "        self._loaded_datasets = {}\n",
    "    \n",
    "    def get_available_datasets(self):\n",
    "        \"\"\"Return a list of available datasets.\"\"\"\n",
    "        return self.available_datasets\n",
    "    \n",
    "    def load_dataset(self, dataset_name):\n",
    "        \"\"\"\n",
    "        Load and preprocess a dataset.\n",
    "        \n",
    "        Args:\n",
    "            dataset_name (str): Name of the dataset to load\n",
    "            \n",
    "        Returns:\n",
    "            dict: Information about the loaded dataset and the data itself\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load the dataset using the existing function\n",
    "            train_features, train_targets, val_features, val_targets, test_features, test_targets = lab3.load_dataset(dataset_name)\n",
    "            \n",
    "            # Store the dataset\n",
    "            self._loaded_datasets[dataset_name] = {\n",
    "                \"train_features\": train_features,\n",
    "                \"train_targets\": train_targets,\n",
    "                \"validation_features\": val_features,\n",
    "                \"validation_targets\": val_targets,\n",
    "                \"test_features\": test_features,\n",
    "                \"test_targets\": test_targets,\n",
    "            }\n",
    "            \n",
    "            # Return information about the loaded dataset\n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"dataset_name\": dataset_name,\n",
    "                \"train_samples\": train_features.shape[0],\n",
    "                \"validation_samples\": val_features.shape[0],\n",
    "                \"test_samples\": test_features.shape[0],\n",
    "                \"feature_dim\": train_features.shape[1]\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"message\": f\"Failed to load dataset '{dataset_name}': {str(e)}\"\n",
    "            }\n",
    "    \n",
    "    def get_dataset(self, dataset_name):\n",
    "        \"\"\"\n",
    "        Retrieve a previously loaded dataset.\n",
    "        \n",
    "        Args:\n",
    "            dataset_name (str): Name of the dataset to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            dict: The dataset components or None if not found\n",
    "        \"\"\"\n",
    "        return self._loaded_datasets.get(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422b98c5",
   "metadata": {},
   "source": [
    "### InterfaceAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caebf674",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32)\n",
    "model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create a simple text-generation pipeline\n",
    "llm_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=200,\n",
    "    do_sample=True,\n",
    "    temperature=0.4,\n",
    ")\n",
    "\n",
    "# Wrapper function for the local model pipeline\n",
    "def local_model_generate(prompt):\n",
    "    output = llm_pipeline(prompt)[0][\"generated_text\"]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df704266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent definition\n",
    "class InterfaceAgent(ConversableAgent):\n",
    "    def __init__(self, name, **kwargs):\n",
    "        super().__init__(name, **kwargs)\n",
    "        \n",
    "        self.dataset_loader_agent = None\n",
    "        self.model_trainer_agent = None\n",
    "\n",
    "    def set_dataset_loader_agent(self, agent):\n",
    "        self.dataset_loader_agent = agent\n",
    "        \n",
    "    def set_model_trainer_agent(self, agent):\n",
    "        self.model_trainer_agent = agent\n",
    "    \n",
    "    def generate_reply(self, messages):\n",
    "        \"\"\"\n",
    "        There is a lot of obtuse text parsing and such here. I would describe the code as \"technically functional\".\n",
    "        This is becasue the LLM is so limited. With a better LLM, and/or ideally one trained explicitly for \n",
    "        agentic implementation, this would be much cleaner and more flexible. \n",
    "        C.f. retrieval augmented generation, etc.\n",
    "        \"\"\"\n",
    "        # Extract the latest user message\n",
    "        user_message = messages[-1][\"content\"]\n",
    "        \n",
    "        # Check if the message is a \"get dataset\" command\n",
    "        # get dataset <dataset_name>\n",
    "        if \"get dataset\" in user_message:\n",
    "            dataset_name = user_message.replace(\"get dataset\", \"\").strip()\n",
    "            \n",
    "            # Use the DatasetLoaderAgent to load the dataset\n",
    "            result = self.dataset_loader_agent.load_dataset(dataset_name)\n",
    "\n",
    "            # Check if the dataset was successfully loaded\n",
    "            if result[\"status\"] == \"success\":\n",
    "                response_text = (\n",
    "                    f\"Successfully loaded dataset '{result[\"dataset_name\"]}'.\\n\"\n",
    "                    f\"Training samples: {result['train_samples']}, \"\n",
    "                    f\"Validation samples: {result['validation_samples']}, \"\n",
    "                    f\"Test samples: {result['test_samples']}, \"\n",
    "                    f\"Feature dimension: {result['feature_dim']}.\"\n",
    "                )\n",
    "            else:\n",
    "                response_text = f\"Failed to load dataset '{dataset_name}': {result['message']}\"\n",
    "        \n",
    "        # Check if the message is a \"query\" command\n",
    "        # query <dataset_name>\n",
    "        elif \"query\" in user_message:\n",
    "            query = user_message.replace(\"query\", \"\").strip()\n",
    "            \n",
    "            # Use the DatasetLoaderAgent to retrieve the dataset\n",
    "            # Assuming the query is in the format \"query <dataset_name>\"\n",
    "            # In principle, this is where you could have an LLM parse the command\n",
    "            dataset_name = query.split()[0]\n",
    "            dataset = self.dataset_loader_agent.get_dataset(dataset_name)\n",
    "    \n",
    "            if dataset:\n",
    "                # Prepare a prompt with dataset information\n",
    "                prompt = (\n",
    "                    f\"The dataset '{dataset_name}' has been loaded. \"\n",
    "                    f\"The first two rows of the training features are:\\n\"\n",
    "                    f\"{dataset['train_features'][:2]}.\\n\"\n",
    "                    \"Short, concise description of the dataset:\\n\"\n",
    "                )\n",
    "                # Generate a description using the local model\n",
    "                response_text = local_model_generate(prompt)\n",
    "            else:\n",
    "                response_text = f\"Dataset '{dataset_name}' is not loaded or does not exist.\"\n",
    "                \n",
    "            \n",
    "        elif \"train\" in user_message:\n",
    "            # Should be of the form \"train <dataset_name> <model_type>\"\n",
    "            # Again, this is where a specialized LLM would be really useful\n",
    "            # Model type\n",
    "            parts = user_message.split()\n",
    "            dataset_name = parts[1]\n",
    "            \n",
    "            # Use the ModelTrainerAgent to train the model\n",
    "            if self.model_trainer_agent is not None:\n",
    "                # If the user specified an available model type, use that\n",
    "                result = self.model_trainer_agent.train_model(dataset_name)\n",
    "                \n",
    "                if result[\"status\"] == \"success\":\n",
    "                    response_text = f\"Successfully trained the model.\"\n",
    "                else:\n",
    "                    response_text = f\"Failed to train the model: {result['response']}\"\n",
    "            else:\n",
    "                response_text = \"Model trainer agent is not set.\"\n",
    "        \n",
    "        # No special command, just a normal message. Can just use the model as a chatbot\n",
    "        else:\n",
    "            response_text = local_model_generate(user_message)\n",
    "        \n",
    "        # Return the response in the Autogen format\n",
    "        return {\"role\": \"assistant\", \"content\": response_text}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a59fe6",
   "metadata": {},
   "source": [
    "## Load a dataset\n",
    "\n",
    "Have the agents load a dataset and describe it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15ef3bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded dataset 'mnist'.\n",
      "Training samples: 800, Validation samples: 200, Test samples: 100, Feature dimension: 784.\n",
      "\n",
      "The dataset 'mnist' has been loaded. The first two rows of the training features are:\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]].\n",
      "Short, concise description of the dataset:\n",
      "The MNIST dataset contains 60,000 28x28 grayscale images of handwritten digits. The dataset is split into a training set of 60,000 images and a test set of 10,000 images. The training set is used to train the neural network, while the test set is used to evaluate the performance of the network. The dataset is used in many machine learning and deep learning applications, including image classification, handwritten digit recognition, and image denoising.\n",
      "\n",
      "4. Preprocessing the dataset:\n",
      "Load the dataset using the `load_data` function. The function takes the path to the dataset as an argument and returns a pandas DataFrame containing the training and test sets.\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "\n",
      "df = pd.read_csv('mnist.csv')\n",
      "```\n",
      "\n",
      "Extract the features and labels from the DataFrame.\n",
      "\n",
      "```python\n",
      "\n",
      "Why don't whales have feet?\n",
      "\n",
      "Whales are marine mammals and have no feet. They have a tail fin instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create instances of the agents\n",
    "dataset_loader_agent = DatasetLoaderAgent(name=\"DatasetLoader\")\n",
    "local_agent = InterfaceAgent(name=\"LocalAgent\")\n",
    "local_agent.set_dataset_loader_agent(dataset_loader_agent)\n",
    "\n",
    "# Simulate a conversation\n",
    "messages = [{\"role\": \"user\", \"content\": \"get dataset mnist\"}]\n",
    "\n",
    "# InterfaceAgent processes the first message\n",
    "response = local_agent.generate_reply(messages)\n",
    "print(response[\"content\"])\n",
    "print()\n",
    "\n",
    "# Add a second message to the conversation\n",
    "messages.append({\"role\": \"user\", \"content\": \"query mnist\"})\n",
    "\n",
    "# InterfaceAgent processes the second message\n",
    "response = local_agent.generate_reply(messages)\n",
    "print(response[\"content\"])\n",
    "print()\n",
    "\n",
    "# You can also use the model as a chatbot\n",
    "messages.append({\"role\": \"user\", \"content\": \"Why don't whales have feet?\"})\n",
    "response = local_agent.generate_reply(messages)\n",
    "print(response[\"content\"])\n",
    "print()\n",
    "\n",
    "# Note that the output is not reliable at all since the model is tiny. Sometimes it's surprisingly good though"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbd182d",
   "metadata": {},
   "source": [
    "## Model Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f14e0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainerAgent(ConversableAgent):\n",
    "    \"\"\"\n",
    "    An agent that selects, creates, and trains a classification model based on the user's query.\n",
    "    \"\"\"\n",
    "    def __init__(self, name=\"ModelTrainer\", dataset_loader_agent=None, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.dataset_loader_agent = dataset_loader_agent\n",
    "        \n",
    "        self.model, self.train_loss_list, self.val_accuracy_list, self.test_accuracy = None, None, None, None\n",
    "        \n",
    "        self.available_models = [\"linear\"]\n",
    "        \n",
    "\n",
    "    def train_model(self, dataset_name):\n",
    "        \"\"\"\n",
    "        Parse the query, select the model, and train it on the dataset.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            model_type = \"linear\"\n",
    "\n",
    "            # Retrieve the dataset from the DatasetLoaderAgent\n",
    "            dataset = self.dataset_loader_agent.get_dataset(dataset_name)\n",
    "            if not dataset:\n",
    "                return f\"Dataset '{dataset_name}' is not loaded or does not exist.\"\n",
    "\n",
    "            # Extract dataset components\n",
    "            train_features = dataset[\"train_features\"]\n",
    "            train_targets = dataset[\"train_targets\"]\n",
    "            val_features = dataset[\"validation_features\"]\n",
    "            val_targets = dataset[\"validation_targets\"]\n",
    "            test_features = dataset[\"test_features\"]\n",
    "            test_targets = dataset[\"test_targets\"]\n",
    "\n",
    "            # Select the model based on the model type\n",
    "            if len(train_features.shape) == 3:\n",
    "                indim = train_features.shape[1]*train_features.shape[2]\n",
    "            else:\n",
    "                indim = train_features.shape[1]\n",
    "            \n",
    "            # Training printing\n",
    "            print(f\"Training model of type '{model_type}' on dataset '{dataset_name}' with input dimension {indim}.\")\n",
    "            print(f\"Training features shape: {train_features.shape}\")\n",
    "            print(f\"Training targets shape: {train_targets.shape}\")\n",
    "            \n",
    "            \n",
    "            model = mnistClassification(input_dim=indim, output_dim=10)\n",
    "\n",
    "            # Train the model\n",
    "            self.model, self.train_loss_list, self.val_accuracy_list, self.test_accuracy = lab3.train_model(\n",
    "                model, train_features, train_targets, val_features, val_targets,\n",
    "                test_features=test_features, test_targets=test_targets\n",
    "            )\n",
    "\n",
    "            # Summarize the training results\n",
    "            response = (\n",
    "                f\"Model '{model_type}' trained successfully on dataset '{dataset_name}'.\\n\"\n",
    "                f\"Final validation accuracy: {self.val_accuracy_list[-1]:.4f}\\n\"\n",
    "                f\"Test accuracy: {self.test_accuracy:.4f}\"\n",
    "            )\n",
    "            print(\"Training response:\", response)\n",
    "            \n",
    "            result = {\n",
    "                \"role\": \"assistant\",\n",
    "                \"status\": \"success\",\n",
    "                \"model\": self.model,\n",
    "                \"train_loss_list\": self.train_loss_list,\n",
    "                \"val_accuracy_list\": self.val_accuracy_list,\n",
    "                \"test_accuracy\": self.test_accuracy,\n",
    "                \"response\": response\n",
    "            }\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        except Exception as e:\n",
    "            return {\"status\": \"error\", \"response\": f\"An error occurred during training: {str(e)}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34e68c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded dataset 'mnist'.\n",
      "Training samples: 800, Validation samples: 200, Test samples: 100, Feature dimension: 784.\n",
      "\n",
      "Training model of type 'linear' on dataset 'mnist' with input dimension 784.\n",
      "Training features shape: (800, 784)\n",
      "Training targets shape: (800,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:01<00:00, 79.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training response: Model 'linear' trained successfully on dataset 'mnist'.\n",
      "Final validation accuracy: 0.9150\n",
      "Test accuracy: 0.8700\n",
      "Successfully trained the model.\n",
      "\n",
      "mnistClassification(\n",
      "  (layer1): Linear(in_features=784, out_features=200, bias=True)\n",
      "  (layer2): Linear(in_features=200, out_features=10, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create instances of the agents\n",
    "dataset_loader_agent = DatasetLoaderAgent(name=\"DatasetLoader\")\n",
    "model_trainer_agent = ModelTrainerAgent(name=\"ModelTrainer\", dataset_loader_agent=dataset_loader_agent)\n",
    "local_agent = InterfaceAgent(name=\"LocalAgent\")\n",
    "\n",
    "local_agent.set_dataset_loader_agent(dataset_loader_agent)\n",
    "local_agent.set_model_trainer_agent(model_trainer_agent)\n",
    "\n",
    "# Simulate a conversation\n",
    "messages = [{\"role\": \"user\", \"content\": \"get dataset mnist\"}]\n",
    "response = local_agent.generate_reply(messages)\n",
    "print(response[\"content\"])\n",
    "print()\n",
    "\n",
    "# Add a training command\n",
    "# \"random\" just to force the agents to talk to each other\n",
    "messages.append({\"role\": \"user\", \"content\": \"train mnist\"})\n",
    "response = local_agent.generate_reply(messages)\n",
    "print(response[\"content\"])\n",
    "print()\n",
    "\n",
    "# And you now have a trained model!\n",
    "print(model_trainer_agent.model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "581f5f99",
   "metadata": {},
   "source": [
    "# Agent Lab Draft 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d682e0d0",
   "metadata": {},
   "source": [
    "## Preliminary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4a3fe1",
   "metadata": {},
   "source": [
    "### Packages to install\n",
    "\n",
    "- ucimlrepo (for grabbing datasets)\n",
    "- transformers\n",
    "- autogen\n",
    "- jupyter (if you aren't on the latest version, there's a dependency in tqdm that complains)\n",
    "\n",
    "#### Install with pip\n",
    "\n",
    "- pip install ucimlrepo transformers autogen jupyter\n",
    "\n",
    "### From lab 3\n",
    "\n",
    "- pandas, numpy, matplotlib.pyplot, seaborn, tqdm, torch, sklearn\n",
    "\n",
    "#### Install everything with pip\n",
    "\n",
    "- pip install ucimlrepo transformers autogen pandas numpy matplotlib seaborn tqdm torch scikit-learn jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac90980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need to install stuff on colab\n",
    "#!pip install ucimlrepo transformers autogen pandas numpy matplotlib seaborn tqdm torch scikit-learn jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54903e1",
   "metadata": {},
   "source": [
    "### Intro\n",
    "\n",
    "The goal of this lab is to give you an idea of how you could use agents to help with physics tasks. It will also introduce you to AutoGen, one of the more popular frameworks at the moment for designing custom agentic workflows. We won't make use of all the tools it provides, just the very basics. Note also that many of the most interesting things one can do with AI-powered agents (see topics like retreival augmented generation (RAG)) require very large models to be performant, and many techniques require continuous/repeated training. This means large resource requirements, so for this lab we will just be using a very small LLM (Llama: TinyLlama-1.1B-Chat-v1.0). The results from this are nowhere near as good as something like chatGPT, but it should give you an idea of how a more advanced model (or models) might be able to do something really helpful/cool. This is an area of current research, so it will be interesting to see what they can do!\n",
    "\n",
    "Also because of the limited size of the model, the text parsing needs to be very mechanical, and in some places a bit obtuse. The better (and ideally specially trained for an agentic workflow, see RAG) your model is, the more this can be relaxed. If you're interested in working with agents in a more user-friendly way (hiding a lot of the mechanics that are on display here), check out sites like n8n or Google Gemini (be aware that these can require you to provide a lot of permissions). You can also feel free to replace TinyLlama with a call to a larger model using API keys if you have some.\n",
    "\n",
    "Finally, we would like to emphasize the use of copilot/similar tools for this lab in particular. These are agents too! And they are definitely the most performant agents you have easy access to. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3f282e",
   "metadata": {},
   "source": [
    "## Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bad3ed",
   "metadata": {},
   "source": [
    "- Do everything with mnist, then try adding solar_flare\n",
    "    - This is where agents can be helpful, since they can analyze a dataset you've never seen before and take a first crack at it much faster than you can"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d391c968",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835b6358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from autogen import ConversableAgent, AssistantAgent\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ucimlrepo import fetch_ucirepo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e85da87",
   "metadata": {},
   "source": [
    "### Lab 3 code (minimize this)\n",
    "\n",
    "- Just packaged into functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f462f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST Dataset in Numpy\n",
    "def load_mnist_data():\n",
    "    # 1000 training samples where each sample feature is a greyscale image with shape (28, 28)\n",
    "    # 1000 training targets where each target is an integer indicating the true digit\n",
    "    mnist_train_features = np.load('mnist_train_features.npy') \n",
    "    mnist_train_targets = np.load('mnist_train_targets.npy')\n",
    "\n",
    "    # 100 testing samples + targets\n",
    "    mnist_test_features = np.load('mnist_test_features.npy')\n",
    "    mnist_test_targets = np.load('mnist_test_targets.npy')\n",
    "\n",
    "    # Print the dimensions of training sample features/targets\n",
    "    #print(mnist_train_features.shape, mnist_train_targets.shape)\n",
    "    # Print the dimensions of testing sample features/targets\n",
    "    #print(mnist_test_features.shape, mnist_test_targets.shape)\n",
    "    \n",
    "    return mnist_train_features, mnist_train_targets, mnist_test_features, mnist_test_targets\n",
    "\n",
    "\n",
    "def flatten_features(features):\n",
    "    # Flatten the features from (28, 28) to (784,)\n",
    "    return features.reshape(features.shape[0], -1)\n",
    "\n",
    "def scale_features(features):\n",
    "    scaler = StandardScaler()\n",
    "    return scaler.fit_transform(features)\n",
    "\n",
    "# More general function to load datasets, including solar_flare\n",
    "def load_dataset(dataset_name: str = \"\"):\n",
    "    if dataset_name == \"mnist\":\n",
    "        train_features, train_targets, test_features, test_targets = load_mnist_data()\n",
    "        train_features = flatten_features(train_features)\n",
    "        test_features = flatten_features(test_features)\n",
    "        train_features = scale_features(train_features)\n",
    "        test_features = scale_features(test_features)\n",
    "        \n",
    "    elif dataset_name == \"solar_flare\":\n",
    "        # Load the solar flare dataset\n",
    "        solar_flare = fetch_ucirepo(id=89)\n",
    "        \n",
    "        # Simplifying slightly for the sake of this example\n",
    "        solar_flare.data.targets = solar_flare.data.targets['severe flares']\n",
    "        \n",
    "        # Split the solar flare dataset into train and test sets (90:10 split)\n",
    "        train_features, test_features, train_targets, test_targets = train_test_split(\n",
    "            solar_flare.data.features, solar_flare.data.targets, test_size=0.1, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Onehot encode modified Zurich class, largest spot size, spot distribution\n",
    "        onehot_columns = [\"modified Zurich class\", \"largest spot size\", \"spot distribution\"]\n",
    "        for col in onehot_columns:\n",
    "            onehot = pd.get_dummies(train_features[col], prefix=col)\n",
    "            train_features = pd.concat([train_features, onehot], axis=1)\n",
    "            train_features.drop(col, axis=1, inplace=True)\n",
    "            \n",
    "            onehot = pd.get_dummies(test_features[col], prefix=col)\n",
    "            test_features = pd.concat([test_features, onehot], axis=1)\n",
    "            test_features.drop(col, axis=1, inplace=True)\n",
    "        \n",
    "        # Scale the features\n",
    "        train_features = scale_features(train_features)\n",
    "        test_features = scale_features(test_features)\n",
    "        \n",
    "        # Convert targets to numpy arrays\n",
    "        train_targets = train_targets.to_numpy()\n",
    "        test_targets = test_targets.to_numpy()\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset: {dataset_name}\")\n",
    "        \n",
    "    # train-test split\n",
    "    train_features, val_features, train_targets, val_targets = train_test_split(train_features, train_targets, test_size=0.2)\n",
    "    \n",
    "    return train_features, train_targets, val_features, val_targets, test_features, test_targets\n",
    "\n",
    "\n",
    "# Train\n",
    "def train_model(model, train_features, train_targets, validation_features, validation_targets, \n",
    "                test_features=None, test_targets=None, learning_rate=0.0015, epochs=80, batch_size=64):\n",
    "    \"\"\"\n",
    "    Train a neural network model on the provided data.\n",
    "    \n",
    "    Parameters:\n",
    "        model: PyTorch model to train\n",
    "        train_features: Training features as numpy array\n",
    "        train_targets: Training targets as numpy array\n",
    "        validation_features: Validation features as numpy array\n",
    "        validation_targets: Validation targets as numpy array\n",
    "        test_features: Test features as numpy array (optional)\n",
    "        test_targets: Test targets as numpy array (optional)\n",
    "        learning_rate: Learning rate for optimizer\n",
    "        epochs: Number of training epochs\n",
    "        batch_size: Batch size for training\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (trained model, training loss list, validation accuracy list)\n",
    "    \"\"\"\n",
    "    # Initialize tracking lists\n",
    "    train_loss_list = np.zeros(epochs)\n",
    "    validation_accuracy_list = np.zeros(epochs)\n",
    "    \n",
    "    # Convert numpy arrays to PyTorch tensors\n",
    "    train_inputs = torch.from_numpy(train_features).float()\n",
    "    train_targets = torch.from_numpy(train_targets).long()\n",
    "    \n",
    "    validation_inputs = torch.from_numpy(validation_features).float()\n",
    "    validation_targets = torch.from_numpy(validation_targets).long()\n",
    "    \n",
    "    if test_features is not None and test_targets is not None:\n",
    "        test_inputs = torch.from_numpy(test_features).float()\n",
    "        test_targets = torch.from_numpy(test_targets).long()\n",
    "        test_dataset = TensorDataset(test_inputs, test_targets)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_dataset = TensorDataset(train_inputs, train_targets)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    validation_dataset = TensorDataset(validation_inputs, validation_targets)\n",
    "    validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Setup optimizer and scheduler\n",
    "    loss_func = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    \n",
    "    # Move model to GPU if available\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        train_inputs = train_inputs.cuda()\n",
    "        validation_inputs = validation_inputs.cuda()\n",
    "        validation_targets = validation_targets.cuda()\n",
    "    \n",
    "    # Training Loop\n",
    "    for epoch in tqdm.trange(epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for batch_inputs, batch_targets in train_loader:\n",
    "            if torch.cuda.is_available():\n",
    "                batch_inputs, batch_targets = batch_inputs.cuda(), batch_targets.cuda()\n",
    "            \n",
    "            optimizer.zero_grad()  # Reset gradients to zero\n",
    "            outputs = model(batch_inputs)  # Forward pass with current batch\n",
    "            loss = loss_func(outputs, batch_targets)  # Compute loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Update weights\n",
    "            \n",
    "            running_loss += loss.item() * batch_inputs.size(0)\n",
    "        \n",
    "        # Store average epoch loss\n",
    "        train_loss_list[epoch] = running_loss / len(train_dataset)\n",
    "        scheduler.step()  # Update learning rate with cosine annealing\n",
    "        \n",
    "        # Compute Validation Accuracy\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for val_inputs, val_targets in validation_loader:\n",
    "                if torch.cuda.is_available():\n",
    "                    val_inputs, val_targets = val_inputs.cuda(), val_targets.cuda()\n",
    "                outputs = model(val_inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += val_targets.size(0)\n",
    "                correct += (predicted == val_targets).sum().item()\n",
    "            \n",
    "            validation_accuracy_list[epoch] = correct / total\n",
    "    \n",
    "    # Compute test accuracy if test data is provided\n",
    "    test_accuracy = None\n",
    "    if test_features is not None and test_targets is not None:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for test_inputs, test_targets in test_loader:\n",
    "                if torch.cuda.is_available():\n",
    "                    test_inputs, test_targets = test_inputs.cuda(), test_targets.cuda()\n",
    "                outputs = model(test_inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += test_targets.size(0)\n",
    "                correct += (predicted == test_targets).sum().item()\n",
    "            \n",
    "            test_accuracy = correct / total\n",
    "    \n",
    "    return model, train_loss_list, validation_accuracy_list, test_accuracy\n",
    "\n",
    "\n",
    "# Visualize and evaluate\n",
    "def visualize_training(train_loss_list, validation_accuracy_list):\n",
    "    \"\"\"\n",
    "    Visualize training loss and validation accuracy.\n",
    "    \n",
    "    Parameters:\n",
    "        train_loss_list: List of training losses\n",
    "        validation_accuracy_list: List of validation accuracies\n",
    "    \"\"\"\n",
    "    plt.figure(figsize = (12, 6))\n",
    "\n",
    "    # Visualize training loss with respect to iterations (1 iteration -> single batch)\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(train_loss_list, linewidth = 3)\n",
    "    plt.ylabel(\"training loss\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "    sns.despine()\n",
    "\n",
    "    # Visualize validation accuracy with respect to epochs\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(validation_accuracy_list, linewidth = 3, color = 'gold')\n",
    "    plt.ylabel(\"validation accuracy\")\n",
    "    sns.despine()\n",
    "    \n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c39b73",
   "metadata": {},
   "source": [
    "## Define Model(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0e4975",
   "metadata": {},
   "source": [
    "#### Possible tasks\n",
    "- Make the convolutional model with copilot\n",
    "    - Or both, even\n",
    "- Replace calls for the self.description property with a request for the main agent to describe the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8355384",
   "metadata": {},
   "outputs": [],
   "source": [
    "class mnistClassification(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim): # Feel free to add hidden_dim as parameters here\n",
    "        super(mnistClassification, self).__init__()\n",
    "        \n",
    "        self.description = \"Simple FCN with 2 layers, ReLU activation, and dropout\"\n",
    "        \n",
    "        #self.layerconv1 = torch.nn.Conv1d(in_channels=1, out_channels=1, kernel_size=5, stride=2, padding=2)\n",
    "        self.layer1 = torch.nn.Linear(input_dim, 200) # First layer\n",
    "        self.layer2 = torch.nn.Linear(200, output_dim) # Second layer\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dropout = torch.nn.Dropout(p=0.2) # Dropout layer with 20% dropout rate\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = torch.nn.functional.softmax(x, dim=1)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "# Uses convolutional layers\n",
    "class mnistConvClassification(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, input_channels,):\n",
    "        super(mnistConvClassification, self).__init__()\n",
    "        \n",
    "        self.description = \"Complex CNN with 2 convolutional layers, 2 linear layers, ReLU activation, and dropout\"\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.input_channels = input_channels\n",
    "        self.height = int((self.input_dim / input_channels) ** 0.5)  # Infer height and width from input_dim\n",
    "        self.width = self.height\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=input_channels, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = torch.nn.Linear(64 * 7 * 7, 128)  # Assuming input images are 28x28\n",
    "        self.fc2 = torch.nn.Linear(128, output_dim)\n",
    "        \n",
    "        # Activation and dropout\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dropout = torch.nn.Dropout(p=0.3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Reshape the input to match the expected dimensions for Conv2d\n",
    "        if len(x.shape) != 3:\n",
    "            x = x.view(-1, self.input_channels, self.height, self.width)\n",
    "        \n",
    "        # Convolutional layers with ReLU and pooling\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Flatten the output for the fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected layers with ReLU and dropout\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return torch.nn.functional.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1b73ea",
   "metadata": {},
   "source": [
    "# Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696d2898",
   "metadata": {},
   "source": [
    "### DatasetLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ce5141",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetLoaderAgent(AssistantAgent):\n",
    "    \"\"\"\n",
    "    An agent that specializes in loading and preprocessing datasets.\n",
    "    \"\"\"\n",
    "    def __init__(self, name=\"DatasetLoader\", **kwargs):\n",
    "        self.available_datasets = [\"mnist\", \"solar_flare\"]\n",
    "        system_message = (\n",
    "            \"I am a dataset loading assistant. I can load and preprocess various datasets for machine learning tasks. \"\n",
    "            f\"Currently I support: {', '.join(self.available_datasets)}. \"\n",
    "        )\n",
    "        super().__init__(name=name, system_message=system_message, **kwargs)\n",
    "        \n",
    "        # Store loaded datasets\n",
    "        self._loaded_datasets = {}\n",
    "    \n",
    "    def get_available_datasets(self):\n",
    "        \"\"\"Return a list of available datasets.\"\"\"\n",
    "        return self.available_datasets\n",
    "    \n",
    "    def load_dataset(self, dataset_name):\n",
    "        \"\"\"\n",
    "        Load and preprocess a dataset.\n",
    "        \n",
    "        Args:\n",
    "            dataset_name (str): Name of the dataset to load\n",
    "            \n",
    "        Returns:\n",
    "            dict: Information about the loaded dataset and the data itself\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load the dataset using the existing function\n",
    "            train_features, train_targets, val_features, val_targets, test_features, test_targets = load_dataset(dataset_name)\n",
    "            \n",
    "            # Store the dataset\n",
    "            self._loaded_datasets[dataset_name] = {\n",
    "                \"train_features\": train_features,\n",
    "                \"train_targets\": train_targets,\n",
    "                \"validation_features\": val_features,\n",
    "                \"validation_targets\": val_targets,\n",
    "                \"test_features\": test_features,\n",
    "                \"test_targets\": test_targets,\n",
    "            }\n",
    "            \n",
    "            # Return information about the loaded dataset\n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"dataset_name\": dataset_name,\n",
    "                \"train_samples\": train_features.shape[0],\n",
    "                \"validation_samples\": val_features.shape[0],\n",
    "                \"test_samples\": test_features.shape[0],\n",
    "                \"feature_dim\": train_features.shape[1]\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"message\": f\"Failed to load dataset '{dataset_name}': {str(e)}\"\n",
    "            }\n",
    "    \n",
    "    def get_dataset(self, dataset_name):\n",
    "        \"\"\"\n",
    "        Retrieve a previously loaded dataset.\n",
    "        \n",
    "        Args:\n",
    "            dataset_name (str): Name of the dataset to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            dict: The dataset components or None if not found\n",
    "        \"\"\"\n",
    "        return self._loaded_datasets.get(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea863d2",
   "metadata": {},
   "source": [
    "### InterfaceAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c18ca87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32)\n",
    "model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create a simple text-generation pipeline\n",
    "llm_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=200,\n",
    "    do_sample=True,\n",
    "    temperature=0.4,\n",
    ")\n",
    "\n",
    "# Wrapper function for the local model pipeline\n",
    "def local_model_generate(prompt):\n",
    "    output = llm_pipeline(prompt)[0][\"generated_text\"]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3ac639",
   "metadata": {},
   "source": [
    "#### Possible Tasks\n",
    "\n",
    "- Implementing query\n",
    "- Implementing having the agents converse to decide on the model type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a136da56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent definition\n",
    "class InterfaceAgent(ConversableAgent):\n",
    "    def __init__(self, name, **kwargs):\n",
    "        super().__init__(name, **kwargs)\n",
    "        \n",
    "        self.dataset_loader_agent = None\n",
    "        self.model_trainer_agent = None\n",
    "\n",
    "    def set_dataset_loader_agent(self, agent):\n",
    "        self.dataset_loader_agent = agent\n",
    "        \n",
    "    def set_model_trainer_agent(self, agent):\n",
    "        self.model_trainer_agent = agent\n",
    "    \n",
    "    def generate_reply(self, messages):\n",
    "        \"\"\"\n",
    "        There is a lot of obtuse text parsing and such here. I would describe the code as \"technically functional\".\n",
    "        This is becasue the LLM is so limited. With a better LLM, and/or ideally one trained explicitly for \n",
    "        agentic implementation, this would be much cleaner and more flexible. \n",
    "        C.f. retrieval augmented generation, etc.\n",
    "        \"\"\"\n",
    "        # Extract the latest user message\n",
    "        user_message = messages[-1][\"content\"]\n",
    "        \n",
    "        # Check if the message is a \"get dataset\" command\n",
    "        if \"get dataset\" in user_message:\n",
    "            dataset_name = user_message.replace(\"get dataset\", \"\").strip()\n",
    "            \n",
    "            # Use the DatasetLoaderAgent to load the dataset\n",
    "            result = self.dataset_loader_agent.load_dataset(dataset_name)\n",
    "\n",
    "            # Check if the dataset was successfully loaded\n",
    "            if result[\"status\"] == \"success\":\n",
    "                response_text = (\n",
    "                    f\"Successfully loaded dataset '{result[\"dataset_name\"]}'.\\n\"\n",
    "                    f\"Training samples: {result['train_samples']}, \"\n",
    "                    f\"Validation samples: {result['validation_samples']}, \"\n",
    "                    f\"Test samples: {result['test_samples']}, \"\n",
    "                    f\"Feature dimension: {result['feature_dim']}.\"\n",
    "                )\n",
    "            else:\n",
    "                response_text = f\"Failed to load dataset '{dataset_name}': {result['message']}\"\n",
    "        \n",
    "        elif \"query\" in user_message:\n",
    "            query = user_message.replace(\"query\", \"\").strip()\n",
    "            \n",
    "            # Use the DatasetLoaderAgent to retrieve the dataset\n",
    "            # Assuming the query is in the format \"query <dataset_name>\"\n",
    "            # In principle, this is where you could have an LLM parse the command\n",
    "            dataset_name = query.split()[0]\n",
    "            dataset = self.dataset_loader_agent.get_dataset(dataset_name)\n",
    "    \n",
    "            if dataset:\n",
    "                # Prepare a prompt with dataset information\n",
    "                prompt = (\n",
    "                    f\"The dataset '{dataset_name}' has been loaded. \"\n",
    "                    f\"The first two rows of the training features are:\\n\"\n",
    "                    f\"{dataset['train_features'][:2]}.\\n\"\n",
    "                    \"Short, concise description of the dataset:\\n\"\n",
    "                )\n",
    "                # Generate a description using the local model\n",
    "                response_text = local_model_generate(prompt)\n",
    "            else:\n",
    "                response_text = f\"Dataset '{dataset_name}' is not loaded or does not exist.\"\n",
    "                \n",
    "            \n",
    "        elif \"train\" in user_message:\n",
    "            # Should be of the form \"train <dataset_name> <model_type>\"\n",
    "            # Again, this is where a specialized LLM would be really useful\n",
    "            # Model type\n",
    "            parts = user_message.split()\n",
    "            dataset_name, modeltype = parts[1], parts[2]\n",
    "            \n",
    "            # Use the ModelTrainerAgent to train the model\n",
    "            if self.model_trainer_agent is not None:\n",
    "                # If the user specified an available model type, use that\n",
    "                if modeltype in self.model_trainer_agent.available_models:\n",
    "                    result = self.model_trainer_agent.train_model_on_query(dataset_name, \"placeholder\", override=modeltype)\n",
    "                    \n",
    "                # Otherwise, use the dataset description to decide on the model type\n",
    "                else:\n",
    "                    message = [{\"role\": \"assistant\", \"content\": f\"query {dataset_name}\"}]\n",
    "                    response = self.generate_reply(message)\n",
    "                    dataset_description = response[\"content\"]\n",
    "                    # Use the dataset description to decide on the model type\n",
    "                    result = self.model_trainer_agent.train_model_on_query(dataset_name, dataset_description)\n",
    "                \n",
    "                if result[\"status\"] == \"success\":\n",
    "                    response_text = f\"Successfully trained the model '{modeltype}'.\"\n",
    "                else:\n",
    "                    response_text = f\"Failed to train the model '{modeltype}': {result['response']}\"\n",
    "            else:\n",
    "                response_text = \"Model trainer agent is not set.\"\n",
    "        \n",
    "        # No special command, just a normal message. Can just use the model as a chatbot\n",
    "        else:\n",
    "            response_text = local_model_generate(user_message)\n",
    "        \n",
    "        # Return the response in the Autogen format\n",
    "        return {\"role\": \"assistant\", \"content\": response_text}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e65149",
   "metadata": {},
   "source": [
    "## Talk to the datasetloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b07504c",
   "metadata": {},
   "source": [
    "#### Possible tasks\n",
    "- All of the below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a50d2ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded dataset 'solar_flare'.\n",
      "Training samples: 1000, Validation samples: 250, Test samples: 139, Feature dimension: 23.\n",
      "\n",
      "The dataset 'solar_flare' has been loaded. The first two rows of the training features are:\n",
      "[[-0.42337369  0.9473887  -0.22049738  1.2453997   0.34940318 -0.16721401\n",
      "   2.1697379  -0.42599822  1.93257561 -0.5471529  -0.29966561 -0.1910085\n",
      "  -0.63475828  2.         -0.16721401 -0.26840486 -0.48495141 -0.75521637\n",
      "  -0.42337369 -0.1954635   1.86125917 -0.90819465 -0.63475828]\n",
      " [ 2.36197954 -0.6860401  -0.22049738 -0.80295507  0.34940318 -0.16721401\n",
      "  -0.46088516 -0.42599822  1.93257561 -0.5471529  -0.29966561 -0.1910085\n",
      "  -0.63475828 -0.5        -0.16721401 -0.26840486 -0.48495141  1.32412385\n",
      "  -0.42337369 -0.1954635  -0.53727069  1.10108555 -0.63475828]].\n",
      "Short, concise description of the dataset:\n",
      "This dataset contains solar flare data from the Solar Dynamics Observatory (SDO) in the form of a pandas dataframe. The data includes the following columns:\n",
      "- 'time': time (in seconds) of the flare\n",
      "- 'flux': intensity of the flare\n",
      "- 'azimuth': azimuth (in degrees) of the flare\n",
      "- 'altitude': altitude (in degrees) of the flare\n",
      "-'speed': speed (in km/s) of the flare\n",
      "-'mag': peak magnetic field strength (in nT)\n",
      "- 'latitude': latitude (in degrees) of the flare\n",
      "- 'longitude': longitude (in degrees) of the flare\n",
      "- 'date': date of the flare, in the format 'YYYY-MM-DD'\n",
      "- 'time_utc': time (in UTC) of the flare, in the format 'YYYY-\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create instances of the agents\n",
    "dataset_loader_agent = DatasetLoaderAgent(name=\"DatasetLoader\")\n",
    "local_agent = InterfaceAgent(name=\"LocalAgent\")\n",
    "local_agent.set_dataset_loader_agent(dataset_loader_agent)\n",
    "\n",
    "# Simulate a conversation\n",
    "messages = [{\"role\": \"user\", \"content\": \"get dataset solar_flare\"}]\n",
    "\n",
    "# InterfaceAgent processes the first message\n",
    "response = local_agent.generate_reply(messages)\n",
    "print(response[\"content\"])\n",
    "print()\n",
    "\n",
    "# Add a second message to the conversation\n",
    "messages.append({\"role\": \"user\", \"content\": \"query solar_flare\"})\n",
    "\n",
    "# InterfaceAgent processes the second message\n",
    "response = local_agent.generate_reply(messages)\n",
    "print(response[\"content\"])\n",
    "print()\n",
    "\n",
    "# Note that the output is not reliable at all since the model is tiny. Sometimes it's surprisingly good though"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55cf5e8",
   "metadata": {},
   "source": [
    "## Model Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc78b0a6",
   "metadata": {},
   "source": [
    "#### Possible Tasks\n",
    "- Implementing everything in train_model_on_query up to setting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed52fd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainerAgent(ConversableAgent):\n",
    "    \"\"\"\n",
    "    An agent that selects, creates, and trains a classification model based on the user's query.\n",
    "    \"\"\"\n",
    "    def __init__(self, name=\"ModelTrainer\", dataset_loader_agent=None, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.dataset_loader_agent = dataset_loader_agent\n",
    "        \n",
    "        self.model, self.train_loss_list, self.val_accuracy_list, self.test_accuracy = None, None, None, None\n",
    "        \n",
    "        self.available_models = [\"linear\", \"conv\"]\n",
    "        \n",
    "\n",
    "    def train_model_on_query(self, dataset_name, query, override=None):\n",
    "        \"\"\"\n",
    "        Parse the query, select the model, and train it on the dataset.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = local_model_generate(f\"Based on the following dataset description, recommend a model type (linear or convolutional): \\n{query}\")\n",
    "            \n",
    "            print(\"Analyzing response:\", response)\n",
    "            \n",
    "            # Parse the query to extract the dataset name and model type\n",
    "            # With a more specialized network, this could be done a lot easier and better\n",
    "            # E.g. here the output is only reversed because the tiny model often just does what it wants at the start\n",
    "            parts = reversed(response.split())\n",
    "            \n",
    "            model_type = \"linear\"  # Default model type\n",
    "            \n",
    "            linear_keywords = [\"linear\", \"fcn\"]\n",
    "            conv_keywords = [\"conv\", \"convolutional\", \"cnn\", \"vision\", \"image\", \"picture\", \"pixel\", \"pictures\", \"images\"]\n",
    "            for word in parts:\n",
    "                if word.lower() in conv_keywords:\n",
    "                    model_type = \"conv\"\n",
    "                    break\n",
    "                elif word.lower() in linear_keywords:\n",
    "                    model_type = \"linear\"\n",
    "                    break\n",
    "\n",
    "            # Retrieve the dataset from the DatasetLoaderAgent\n",
    "            dataset = self.dataset_loader_agent.get_dataset(dataset_name)\n",
    "            if not dataset:\n",
    "                return f\"Dataset '{dataset_name}' is not loaded or does not exist.\"\n",
    "\n",
    "            # Extract dataset components\n",
    "            train_features = dataset[\"train_features\"]\n",
    "            train_targets = dataset[\"train_targets\"]\n",
    "            val_features = dataset[\"validation_features\"]\n",
    "            val_targets = dataset[\"validation_targets\"]\n",
    "            test_features = dataset[\"test_features\"]\n",
    "            test_targets = dataset[\"test_targets\"]\n",
    "\n",
    "            # Select the model based on the model type\n",
    "            if len(train_features.shape) == 3:\n",
    "                indim = train_features.shape[1]*train_features.shape[2]\n",
    "            else:\n",
    "                indim = train_features.shape[1]\n",
    "            \n",
    "            # Training printing\n",
    "            print(f\"Training model of type '{model_type}' on dataset '{dataset_name}' with input dimension {indim}.\")\n",
    "            print(f\"Training features shape: {train_features.shape}\")\n",
    "            print(f\"Training targets shape: {train_targets.shape}\")\n",
    "            \n",
    "            \n",
    "            if model_type == \"linear\" or override == \"linear\":\n",
    "                model = mnistClassification(input_dim=indim, output_dim=10)\n",
    "            elif model_type == \"conv\" or override == \"conv\":\n",
    "                model = mnistConvClassification(input_dim=indim, input_channels=1, output_dim=10)\n",
    "            else: # Fallback\n",
    "                model = mnistConvClassification(input_dim=indim, input_channels=1, output_dim=10)\n",
    "\n",
    "            # Train the model\n",
    "            self.model, self.train_loss_list, self.val_accuracy_list, self.test_accuracy = train_model(\n",
    "                model, train_features, train_targets, val_features, val_targets,\n",
    "                test_features=test_features, test_targets=test_targets\n",
    "            )\n",
    "\n",
    "            # Summarize the training results\n",
    "            response = (\n",
    "                f\"Model '{model_type}' trained successfully on dataset '{dataset_name}'.\\n\"\n",
    "                f\"Final validation accuracy: {self.val_accuracy_list[-1]:.4f}\\n\"\n",
    "                f\"Test accuracy: {self.test_accuracy:.4f}\"\n",
    "            )\n",
    "            print(\"Training response:\", response)\n",
    "            \n",
    "            result = {\n",
    "                \"role\": \"assistant\",\n",
    "                \"status\": \"success\",\n",
    "                \"model\": self.model,\n",
    "                \"train_loss_list\": self.train_loss_list,\n",
    "                \"val_accuracy_list\": self.val_accuracy_list,\n",
    "                \"test_accuracy\": self.test_accuracy,\n",
    "                \"response\": response\n",
    "            }\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        except Exception as e:\n",
    "            return {\"status\": \"error\", \"response\": f\"An error occurred during training: {str(e)}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5ef0c7",
   "metadata": {},
   "source": [
    "#### Possible Tasks\n",
    "- All of the below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81f9fc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded dataset 'solar_flare'.\n",
      "Training samples: 1000, Validation samples: 250, Test samples: 139, Feature dimension: 23.\n",
      "\n",
      "Analyzing response: Based on the following dataset description, recommend a model type (linear or convolutional): \n",
      "The dataset 'solar_flare' has been loaded. The first two rows of the training features are:\n",
      "[[-0.42337369 -0.6860401  -0.22049738 -0.80295507  0.34940318 -0.16721401\n",
      "  -0.46088516 -0.42599822 -0.51744418  1.82764268 -0.29966561 -0.1910085\n",
      "  -0.63475828  2.         -0.16721401 -0.26840486 -0.48495141 -0.75521637\n",
      "  -0.42337369 -0.1954635  -0.53727069  1.10108555 -0.63475828]\n",
      " [-0.42337369  0.9473887  -0.22049738 -0.80295507 -2.86202314 -0.16721401\n",
      "   2.1697379  -0.42599822 -0.51744418 -0.5471529  -0.29966561 -0.1910085\n",
      "   1.57540285 -0.5        -0.16721401 -0.26840486 -0.48495141  1.32412385\n",
      "  -0.42337369 -0.1954635  -0.53727069 -0.90819465  1.57540285]].\n",
      "Short, concise description of the dataset:\n",
      "\n",
      "The dataset'solar_flare' contains 2000 solar flares from 1995 to 2015. The data is in CSV format and contains the following columns:\n",
      "\n",
      "- 'time': time in seconds since epoch when the flare occurred\n",
      "- 'flare_type': type of flare (e.g. C-class, M-class)\n",
      "- 'flare_class': class of flare (e.g. C-class, M-class)\n",
      "-'mag': magnitude of the flare (0-9)\n",
      "- 'flux': flux of the flare in units of 10<sup>-12</sup> erg/cm<sup>2</sup>\n",
      "- 'flux_class': class of the flare in terms of the International Flare Catalogue (e.g. Class C, Class M)\n",
      "- 'flux_class_fraction': fraction of the flare's flux in the Class C or Class M category\n",
      "- 'peak_flux': peak flux of the flare (in units of 10<sup>-12</sup> erg/cm<sup>2</sup>)\n",
      "- 'duration': duration of the flare in seconds\n",
      "- 'duration_class': class of the flare in terms of the International Flare Catalogue (e.g. Class C, Class M)\n",
      "- 'duration_class_fraction': fraction of the flare's duration in the Class C or Class M category\n",
      "- 'energy_class': class of the flare in terms of the International Flare Catalogue (e.g. Class C, Class M)\n",
      "- 'energy_class_fraction': fraction of the flare's energy in the Class C or Class M category\n",
      "-'mag_class': class of the flare\n",
      "Training model of type 'linear' on dataset 'solar_flare' with input dimension 23.\n",
      "Training features shape: (1000, 23)\n",
      "Training targets shape: (1000,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:00<00:00, 93.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training response: Model 'linear' trained successfully on dataset 'solar_flare'.\n",
      "Final validation accuracy: 0.9800\n",
      "Test accuracy: 0.9928\n",
      "Successfully trained the model 'random'.\n",
      "\n",
      "mnistClassification(\n",
      "  (layer1): Linear(in_features=23, out_features=200, bias=True)\n",
      "  (layer2): Linear(in_features=200, out_features=10, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create instances of the agents\n",
    "dataset_loader_agent = DatasetLoaderAgent(name=\"DatasetLoader\")\n",
    "model_trainer_agent = ModelTrainerAgent(name=\"ModelTrainer\", dataset_loader_agent=dataset_loader_agent)\n",
    "local_agent = InterfaceAgent(name=\"LocalAgent\")\n",
    "\n",
    "local_agent.set_dataset_loader_agent(dataset_loader_agent)\n",
    "local_agent.set_model_trainer_agent(model_trainer_agent)\n",
    "\n",
    "# Simulate a conversation\n",
    "messages = [{\"role\": \"user\", \"content\": \"get dataset solar_flare\"}]\n",
    "response = local_agent.generate_reply(messages)\n",
    "print(response[\"content\"])\n",
    "print()\n",
    "\n",
    "# Add a training command\n",
    "# \"random\" just to force the agents to talk to each other\n",
    "messages.append({\"role\": \"user\", \"content\": \"train solar_flare random\"})\n",
    "response = local_agent.generate_reply(messages)\n",
    "print(response[\"content\"])\n",
    "print()\n",
    "\n",
    "# And you now have a trained model!\n",
    "print(model_trainer_agent.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f02416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded dataset 'mnist'.\n",
      "Training samples: 800, Validation samples: 200, Test samples: 100, Feature dimension: 784.\n",
      "\n",
      "Analyzing response: Based on the following dataset description, recommend a model type (linear or convolutional): \n",
      "The dataset 'mnist' has been loaded. The first two rows of the training features are:\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]].\n",
      "Short, concise description of the dataset:\n",
      "\n",
      "MNIST dataset is a widely used dataset for image classification. It is a subset of the MNIST database, which is a collection of handwritten digits. The dataset contains 60,000 labeled images of size 28x28 pixels and 10 classes (0-9 digits). The training set contains 50,000 labeled images, while the test set contains 10,000 unlabeled images. The dataset is used in many deep learning research papers and is a popular benchmark for image classification tasks. The dataset is available in the Keras library and can be loaded using the `load_data()` function. The first two rows of the training features are:\n",
      "\n",
      "```\n",
      "array([[0.       , 0.       , 0.       ,..., 0.       , 0.        ],\n",
      "       [0.       , 0.       , 0.       ,..., 0.       , 0.        ],\n",
      "       [0.       , 0.       , 0.       ,..., 0.       , 0.        ],\n",
      "      ...,\n",
      "       [0.       , 0.       , 0.       ,..., 0.       , 0.        ],\n",
      "       [0.       , 0.       , 0.       ,..., 0.       , 0.        ],\n",
      "       [0.       , 0.       , 0.       ,..., 0.       , 0.        ]])\n",
      "```\n",
      "\n",
      "This dataset is\n",
      "Training model of type 'conv' on dataset 'mnist' with input dimension 784.\n",
      "Training features shape: (800, 784)\n",
      "Training targets shape: (800,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:11<00:00,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training response: Model 'conv' trained successfully on dataset 'mnist'.\n",
      "Final validation accuracy: 0.9300\n",
      "Test accuracy: 0.9700\n",
      "Successfully trained the model 'random'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "messages = [{\"role\": \"user\", \"content\": \"get dataset mnist\"}]\n",
    "response = local_agent.generate_reply(messages)\n",
    "print(response[\"content\"])\n",
    "print()\n",
    "\n",
    "# Should choose the conv model; usually does\n",
    "messages.append({\"role\": \"user\", \"content\": \"train mnist random\"})\n",
    "response = local_agent.generate_reply(messages)\n",
    "print(response[\"content\"])\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
